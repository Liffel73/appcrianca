/**
 * Hook de Reconhecimento de Voz para React Native
 * Grava √°udio e envia para Whisper no backend
 * 
 * Features:
 * - Grava√ß√£o de at√© 6 segundos
 * - Convers√£o para base64
 * - Integra√ß√£o com Whisper
 * - Feedback visual e sonoro
 * - Tratamento de erros robusto
 */

import { useState, useRef, useCallback } from 'react';
import { Audio } from 'expo-av';
import * as FileSystem from 'expo-file-system';
import axios from 'axios';
import { Alert, Platform } from 'react-native';

const API_BASE = 'http://localhost:8000';

interface UseSpeechRecognitionOptions {
  language?: 'pt' | 'en';
  maxDuration?: number; // em milissegundos
  onTranscript?: (text: string) => void;
  onError?: (error: Error) => void;
}

interface UseSpeechRecognitionReturn {
  // Estados
  isRecording: boolean;
  isProcessing: boolean;
  transcript: string | null;
  error: string | null;
  recordingDuration: number;
  
  // Fun√ß√µes
  startRecording: () => Promise<void>;
  stopRecording: () => Promise<string | null>;
  cancelRecording: () => void;
  resetTranscript: () => void;
}

/**
 * Hook principal para reconhecimento de voz
 */
export function useSpeechRecognition(
  options: UseSpeechRecognitionOptions = {}
): UseSpeechRecognitionReturn {
  const {
    language = 'pt',
    maxDuration = 6000, // 6 segundos padr√£o
    onTranscript,
    onError
  } = options;

  // Estados
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [recordingDuration, setRecordingDuration] = useState(0);

  // Refs
  const recordingRef = useRef<Audio.Recording | null>(null);
  const recordingTimerRef = useRef<NodeJS.Timeout | null>(null);
  const durationTimerRef = useRef<NodeJS.Timeout | null>(null);
  const startTimeRef = useRef<number>(0);

  /**
   * Configurar permiss√µes de √°udio
   */
  const setupAudioMode = async () => {
    try {
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
        shouldDuckAndroid: true,
        playThroughEarpieceAndroid: false,
        staysActiveInBackground: true,
      });
    } catch (err) {
      console.error('[SpeechRecognition] Erro ao configurar √°udio:', err);
    }
  };

  /**
   * Converter arquivo para base64
   */
  const fileToBase64 = async (uri: string): Promise<string> => {
    try {
      const base64 = await FileSystem.readAsStringAsync(uri, {
        encoding: FileSystem.EncodingType.Base64,
      });
      return base64;
    } catch (err) {
      console.error('[SpeechRecognition] Erro ao converter para base64:', err);
      throw new Error('Falha ao processar √°udio');
    }
  };

  /**
   * Iniciar grava√ß√£o
   */
  const startRecording = useCallback(async () => {
    try {
      console.log('[SpeechRecognition] üé§ Iniciando grava√ß√£o...');
      
      // Reset estados
      setError(null);
      setTranscript(null);
      setRecordingDuration(0);
      
      // Verificar permiss√µes
      const { status } = await Audio.requestPermissionsAsync();
      if (status !== 'granted') {
        const errorMsg = 'Permiss√£o de microfone negada';
        setError(errorMsg);
        onError?.(new Error(errorMsg));
        Alert.alert(
          'Permiss√£o Necess√°ria',
          'Por favor, permita o acesso ao microfone nas configura√ß√µes do app.'
        );
        return;
      }

      // Configurar modo de √°udio
      await setupAudioMode();

      // Parar grava√ß√£o anterior se houver
      if (recordingRef.current) {
        try {
          await recordingRef.current.stopAndUnloadAsync();
        } catch (err) {
          console.warn('[SpeechRecognition] Aviso ao parar grava√ß√£o anterior:', err);
        }
      }

      // Criar nova grava√ß√£o com qualidade alta
      const recording = new Audio.Recording();
      
      // Op√ß√µes de grava√ß√£o otimizadas para Whisper
      const recordingOptions = {
        android: {
          extension: '.m4a',
          outputFormat: Audio.RECORDING_OPTION_ANDROID_OUTPUT_FORMAT_MPEG_4,
          audioEncoder: Audio.RECORDING_OPTION_ANDROID_AUDIO_ENCODER_AAC,
          sampleRate: 44100,
          numberOfChannels: 1,
          bitRate: 128000,
        },
        ios: {
          extension: '.m4a',
          outputFormat: Audio.RECORDING_OPTION_IOS_OUTPUT_FORMAT_MPEG4AAC,
          audioQuality: Audio.RECORDING_OPTION_IOS_AUDIO_QUALITY_HIGH,
          sampleRate: 44100,
          numberOfChannels: 1,
          bitRate: 128000,
          linearPCMBitDepth: 16,
          linearPCMIsBigEndian: false,
          linearPCMIsFloat: false,
        },
        web: {
          mimeType: 'audio/webm',
          bitsPerSecond: 128000,
        }
      };

      await recording.prepareToRecordAsync(recordingOptions);
      await recording.startAsync();
      
      recordingRef.current = recording;
      setIsRecording(true);
      startTimeRef.current = Date.now();
      
      console.log('[SpeechRecognition] ‚úÖ Grava√ß√£o iniciada');

      // Timer para atualizar dura√ß√£o
      durationTimerRef.current = setInterval(() => {
        const elapsed = Date.now() - startTimeRef.current;
        setRecordingDuration(Math.floor(elapsed / 1000));
      }, 100);

      // Auto-stop ap√≥s maxDuration
      recordingTimerRef.current = setTimeout(async () => {
        console.log('[SpeechRecognition] ‚è±Ô∏è Tempo m√°ximo atingido, parando...');
        await stopRecording();
      }, maxDuration);

    } catch (err) {
      console.error('[SpeechRecognition] ‚ùå Erro ao iniciar grava√ß√£o:', err);
      const errorMsg = 'Falha ao iniciar grava√ß√£o';
      setError(errorMsg);
      setIsRecording(false);
      onError?.(err as Error);
      Alert.alert('Erro', errorMsg);
    }
  }, [maxDuration, onError]);

  /**
   * Parar grava√ß√£o e processar
   */
  const stopRecording = useCallback(async (): Promise<string | null> => {
    try {
      if (!recordingRef.current || !isRecording) {
        console.warn('[SpeechRecognition] Nenhuma grava√ß√£o ativa para parar');
        return null;
      }

      console.log('[SpeechRecognition] üõë Parando grava√ß√£o...');
      setIsRecording(false);
      setIsProcessing(true);

      // Limpar timers
      if (recordingTimerRef.current) {
        clearTimeout(recordingTimerRef.current);
        recordingTimerRef.current = null;
      }
      if (durationTimerRef.current) {
        clearInterval(durationTimerRef.current);
        durationTimerRef.current = null;
      }

      // Parar grava√ß√£o
      await recordingRef.current.stopAndUnloadAsync();
      const uri = recordingRef.current.getURI();
      recordingRef.current = null;

      if (!uri) {
        throw new Error('Nenhum arquivo de √°udio foi gerado');
      }

      console.log('[SpeechRecognition] üìÅ Arquivo salvo em:', uri);

      // Converter para base64
      console.log('[SpeechRecognition] üîÑ Convertendo para base64...');
      const base64Audio = await fileToBase64(uri);
      
      // Criar formato compat√≠vel com o backend (WebM base64)
      const audioData = `data:audio/webm;base64,${base64Audio}`;

      // Enviar para o backend
      console.log('[SpeechRecognition] üì° Enviando para Whisper...');
      const endpoint = language === 'pt' ? '/speech-to-text-pt' : '/speech-to-text';
      
      const response = await axios.post(
        `${API_BASE}${endpoint}`,
        { audio: audioData },
        { 
          timeout: 30000, // 30 segundos de timeout
          headers: {
            'Content-Type': 'application/json'
          }
        }
      );

      const { transcript: text } = response.data;
      
      console.log('[SpeechRecognition] ‚úÖ Transcri√ß√£o recebida:', text);
      setTranscript(text);
      setIsProcessing(false);
      onTranscript?.(text);

      // Limpar arquivo tempor√°rio
      try {
        await FileSystem.deleteAsync(uri, { idempotent: true });
      } catch (err) {
        console.warn('[SpeechRecognition] Aviso ao deletar arquivo tempor√°rio:', err);
      }

      return text;

    } catch (err) {
      console.error('[SpeechRecognition] ‚ùå Erro ao processar grava√ß√£o:', err);
      const errorMsg = axios.isAxiosError(err) 
        ? err.response?.data?.detail || 'Erro ao transcrever √°udio'
        : 'Falha no processamento do √°udio';
      
      setError(errorMsg);
      setIsProcessing(false);
      onError?.(err as Error);
      
      Alert.alert('Erro', errorMsg);
      return null;
    }
  }, [isRecording, language, onTranscript, onError]);

  /**
   * Cancelar grava√ß√£o sem processar
   */
  const cancelRecording = useCallback(() => {
    console.log('[SpeechRecognition] ‚ùå Cancelando grava√ß√£o...');
    
    // Limpar timers
    if (recordingTimerRef.current) {
      clearTimeout(recordingTimerRef.current);
      recordingTimerRef.current = null;
    }
    if (durationTimerRef.current) {
      clearInterval(durationTimerRef.current);
      durationTimerRef.current = null;
    }

    // Parar grava√ß√£o se estiver ativa
    if (recordingRef.current && isRecording) {
      recordingRef.current.stopAndUnloadAsync().catch(err => {
        console.warn('[SpeechRecognition] Aviso ao cancelar grava√ß√£o:', err);
      });
      recordingRef.current = null;
    }

    setIsRecording(false);
    setIsProcessing(false);
    setRecordingDuration(0);
    setError(null);
  }, [isRecording]);

  /**
   * Resetar transcri√ß√£o
   */
  const resetTranscript = useCallback(() => {
    setTranscript(null);
    setError(null);
    setRecordingDuration(0);
  }, []);

  return {
    // Estados
    isRecording,
    isProcessing,
    transcript,
    error,
    recordingDuration,
    
    // Fun√ß√µes
    startRecording,
    stopRecording,
    cancelRecording,
    resetTranscript,
  };
}

/**
 * Componente de bot√£o de grava√ß√£o reutiliz√°vel
 */
export { RecordButton } from './RecordButton';

// Exportar tipos
export type { 
  UseSpeechRecognitionOptions, 
  UseSpeechRecognitionReturn 
};
